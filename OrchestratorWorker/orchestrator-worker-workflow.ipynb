{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80927faf",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9dd337",
   "metadata": {},
   "source": [
    "Orchestrator-Worker Workflow Design Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b667ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c422cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced problem in the field of farming that I can ask a LLM.\"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"openai/gpt-oss-120b\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "question = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346275c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_workers = [\"gpt-5-nano\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\", \"openai/gpt-oss-120b\", \"llama3.2\"]\n",
    "the_orchestrator = \"openai/gpt-oss-120b\"\n",
    "\n",
    "def gpt_nano_use(messages, model_name):\n",
    "    openai = OpenAI()\n",
    "    response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "def claude_use(messages, model_name):\n",
    "    claude = Anthropic()\n",
    "    response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "    answer = response.content[0].text\n",
    "    return answer\n",
    "\n",
    "def gemini_use(messages, model_name):\n",
    "    gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "def gpt_oss_use(messages, model_name):\n",
    "    groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "    response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "def llama_use(messages, model_name):\n",
    "    ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "    response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ecec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = f\"Please break down the following question into sub-problems: {question}.\"\n",
    "task += f\"Then select model that you think will provide the best response quality per sub-problem out of {the_workers}, you are allowed to repeat models if you think that is necessary. \"\n",
    "task += f\"Answer only with the sub-problems and the models that you have selected, no explanation.\"\n",
    "task += f\"Provide answer in JSON format: {{'sub-problem': 'model', 'sub-problem': 'model', ...}}\"\n",
    "messages = [{\"role\": \"user\", \"content\": task}]\n",
    "\n",
    "groq_orchestrator = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "response = groq_orchestrator.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_messages = []\n",
    "answer_combination = []\n",
    "for sub_problem, model in json.loads(answer).items():\n",
    "    backbone_messages.append(f\"Sub-problem: {sub_problem}, Model: {model}\")\n",
    "    if model == \"gpt-5-nano\":\n",
    "        answer_combination.append(gpt_nano_use(messages, model))\n",
    "    elif model == \"claude-sonnet-4-5\":\n",
    "        answer_combination.append(claude_use(messages, model))\n",
    "    elif model == \"gemini-2.5-flash\":\n",
    "        answer_combination.append(gemini_use(messages, model))\n",
    "    elif model == \"openai/gpt-oss-120b\":\n",
    "        answer_combination.append(gpt_oss_use(messages, model))\n",
    "    elif model == \"llama3.2\":\n",
    "        answer_combination.append(llama_use(messages, model))\n",
    "\n",
    "print(answer_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_messages = [{\"role\": \"user\", \"content\": f\"synthesize all of these {answer_combination} and provide the final response to the {question}\"}]\n",
    "response = groq_orchestrator.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=synthesis_messages)\n",
    "final_answer = response.choices[0].message.content\n",
    "display(Markdown(final_answer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ee111",
   "metadata": {},
   "source": [
    "Cross Verify with a Solo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad89bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": f\"provide a response to answer {question}\"}]\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebef43",
   "metadata": {},
   "source": [
    "Key Takeaways\n",
    "\n",
    "1. The amount of raw detail and facts (statistics, data, etc.) that is provided from the workflow incorporating many models is far greater than a single model.\n",
    "2. The solo model is still was more coherent because all the models in the workflow were proposing different strategies, so direction of the initiatize was diluated. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
